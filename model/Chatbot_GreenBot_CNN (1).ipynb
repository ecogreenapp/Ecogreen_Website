{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zWah1OZ-y-E"
      },
      "source": [
        "# Koneksi Google Drive and Folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B4A9dFVuo8X",
        "outputId": "bfa5c242-d048-4523-dac8-720285b59dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_root='/content/drive/MyDrive/Chatbot/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc1HPq2z-4z3"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT1-e6LisLPk",
        "outputId": "0d78e09d-aa4c-4770-daa9-953ada4b67ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "# untuk membuat dan membaca (konsumsi) data JSON.\n",
        "\n",
        "import string\n",
        "# Modul Python String berisi beberapa konstanta, fungsi utilitas, dan kelas untuk manipulasi string.\n",
        "\n",
        "import random\n",
        "# Modul random pada python berfungsi untuk menghasilkan angka acak yang bisa kamu gunakan untuk berbagai keperluan\n",
        "\n",
        "import nltk\n",
        " #  singkatan dari Natural Language Tool Kit, yaitu sebuah library yang digunakan untuk membantu kita dalam bekerja dengan teks.\n",
        "#  Library ini memudahkan kita untuk memproses teks seperti melakukan classification, tokenization, stemming, tagging, parsing, dan semantic reasoning.\n",
        "\n",
        "import numpy as np\n",
        "# mendukung array (termasuk array multidimensi) serta menyediakan koleksi fungsi matematika untuk mengoperasikan array tersebut\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Salah satu modulnya adalah WordNet Lemmatizer yang dapat digunakan untuk melakukan lemmatisasi pada kata-kata.\n",
        "#Lemmatisasi adalah proses mengelompokkan berbagai bentuk infleksi suatu kata sehingga dapat dianalisis sebagai satu item. Lemmatisasi mirip dengan stemming tetapi\n",
        "#membawa konteks pada kata-katanya. Jadi itu menghubungkan kata-kata dengan arti yang mirip dengan satu kata.\n",
        "\n",
        "import tensorflow as tf\n",
        "# Mempersiapkan data latih dan target. Mengonfigurasi parameter pelatihan seperti jumlah epoch, batch size, dan lainnya.\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "# digunakan untuk mengimpor kelas Sequential dari modul keras dalam pustaka tensorflow,\n",
        "#Sequential adalah bagian dari API Keras, yang merupakan antarmuka tingkat tinggi untuk membangun dan melatih model neural network.\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "# Dropout adalah teknik regulasi yang umum digunakan untuk mencegah overfitting dalam model neural network.\n",
        "# from tensorflow.keras.layers import Dense, Dropout digunakan untuk mengimpor dua jenis lapisan (layers)\n",
        "# yang umum digunakan dalam pembangunan model neural network menggunakan pustaka tensorflow.keras\n",
        "\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "# digunakan untuk mengunduh data Punkt, yang merupakan model tokenisasi yang disediakan oleh NLTK (Natural Language Toolkit) di Python.\n",
        "# Tokenisasi adalah proses membagi teks menjadi token atau unit-unit kecil seperti kata atau kalimat. Dengan mengunduh data Punkt\n",
        "# download database corpus dari nltk. Proses ini hanya sekali ketika running awal.\n",
        "\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "#  digunakan untuk mengunduh data WordNet melalui NLTK (Natural Language Toolkit) di Python. WordNet adalah sebuah kamus bahasa Inggris yang\n",
        "#  memiliki struktur hierarki dan mengelompokkan kata-kata berdasarkan makna dan hubungan antar kata tersebut.\n",
        "\n",
        "\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQx5moAd_HGn"
      },
      "source": [
        "# Reading the JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GjyknLtzs9K5"
      },
      "outputs": [],
      "source": [
        "data_file = open(data_root + '/intents.json').read()\n",
        "\n",
        "data = json.loads(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DttsBlXz_Wk_"
      },
      "source": [
        "# Identifikasi Fitur & target model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKLYU1RY8AC_"
      },
      "source": [
        "Penggunaan NLTK: Kode menggunakan modul NLTK untuk tokenisasi kata dan WordNetLemmatizer untuk melakukan lemmatisasi kata-kata. Tokenisasi membantu dalam memecah pola-pola menjadi kata-kata individual, sedangkan lemmatisasi mengonversi kata-kata ke bentuk dasarnya.\n",
        "\n",
        "\n",
        "Menghilangkan tanda baca dan mengurutkan kata-kata: Kode juga menghilangkan tanda baca dan mengurutkan kata-kata yang telah dilematisasi untuk memastikan bahwa setiap kata hanya muncul sekali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI7Zst_a9-Lv"
      },
      "source": [
        "\n",
        "1. case folding = penyeragaman karakter (lower)\n",
        "2. filtering = karakter yang dianggap ilegal\n",
        "3. tokenization = jika tidak melakukan ini maka tidak bisa mengecek setiap kata (tidak bisa detect kata penting / baku) masing2 kata berdiri memisahkan kalimat menjadi kata perkata berdiri sendiri\n",
        "4. slangword convertion = menjadikan kata dasar misal cepat cepet\n",
        "5. stopword removal =menghapus kata imbuhan\n",
        "6. stemming = mengubah imbuhan ke bentuk dasar (imbuhan bahasa indonesia)\n",
        "7. lemmatization = mengubah imbuhan ke bentuk dasar bentuknya kamus (imbuhan bahasa indonesia + inggris kata baku)\n",
        "8. padding = data teks harus sama (deep learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dub7a-1g-R1-"
      },
      "source": [
        "# Feature Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wO3iRtZztfvu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# getting informations from intents.json--\n",
        "words = []     # Daftar kata-kata dari pola-pola\n",
        "classes = []   # Daftar kelas\n",
        "data_x = []    # Pola-pola\n",
        "data_y = []    # Kelas yang sesuai dengan pola-pola\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        # Tokenisasi kata dari setiap pola\n",
        "        tokens = nltk.word_tokenize(pattern)\n",
        "        words.extend(tokens)\n",
        "        data_x.append(pattern)\n",
        "        data_y.append(intent[\"tag\"]),\n",
        "\n",
        "        if intent[\"tag\"] not in classes:\n",
        "          classes.append(intent[\"tag\"])\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Membuat objek lemmatizer dari WordNetLemmatizer\n",
        "\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "# Melakukan lematisasi pada setiap kata dalam daftar 'words' dan mengonversi ke huruf kecil\n",
        "\n",
        "words = sorted(set(words))\n",
        "# Mengurutkan dan menghapus duplikat kata-kata dalam daftar 'words'\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "# Mengurutkan dan menghapus duplikat niat (classes) dalam daftar 'classes'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VInarAZ9_fJo"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xjd-gigkzUs7"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "# Membuat list kosong untuk menyimpan data pelatihan\n",
        "\n",
        "out_empty = [0] * len(classes)\n",
        "# Membuat list kosong berisi nol sebanyak jumlah kelas\n",
        "\n",
        "\n",
        "for idx, doc in enumerate(data_x):\n",
        "# Iterasi melalui setiap dokumen (pola) dalam data_x\n",
        "\n",
        "  bow = []\n",
        "# Inisialisasi BoW (Bag of Words) untuk dokumen saat ini\n",
        "\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "# Melakukan lematisasi dan mengonversi teks ke huruf kecil\n",
        "\n",
        "  for word in words:\n",
        "# Iterasi melalui setiap kata dalam daftar kata-kata yang telah diproses sebelumnya\n",
        "\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "# Jika kata dalam BoW, tambahkan 1, jika tidak, tambahkan 0\n",
        "\n",
        "    output_row = list(out_empty)\n",
        "# Inisialisasi baris output dengan list nol sesuai jumlah kelas\n",
        "\n",
        "    output_row[classes.index(data_y[idx])] = 1\n",
        "# Mengatur elemen sesuai dengan indeks kelas yang sesuai untuk output_row\n",
        "\n",
        "    training.append([bow, output_row])\n",
        "# Menambahkan pasangan BoW dan output_row ke dalam data pelatihan\n",
        "\n",
        "\n",
        "random.shuffle(training)\n",
        "# Mengacak urutan data pelatihan\n",
        "\n",
        "training = np.array(training, dtype=object)\n",
        "# Mengonversi list training ke dalam array NumPy dengan tipe data objek\n",
        "\n",
        "\n",
        "train_x = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:, 1]))\n",
        "\n",
        "# Mengonversi array NumPy ke dalam train_x dan train_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS3HM_Km_kfU"
      },
      "source": [
        "Sequential adalah model neural network yang umum digunakan dalam Keras, yang merupakan bagian dari TensorFlow. Model ini digunakan untuk membangun jaringan neural secara sekuensial, satu lapisan setelah yang lain.\n",
        "\n",
        "Pada model Sequential, setiap layer terhubung ke layer sebelumnya, dan informasi mengalir secara sekuensial melalui model tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWnEXY1-95EK"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Anvpbac0H8a",
        "outputId": "1aa355dd-2e53-42eb-ea4c-f28996b0e441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               38656     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 106)               6890      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53802 (210.16 KB)\n",
            "Trainable params: 53802 (210.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "3932/3932 [==============================] - 15s 4ms/step - loss: 0.7905 - accuracy: 0.7887\n",
            "Epoch 2/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.6453 - accuracy: 0.8464\n",
            "Epoch 3/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.6621 - accuracy: 0.8523\n",
            "Epoch 4/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.6807 - accuracy: 0.8513\n",
            "Epoch 5/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.7017 - accuracy: 0.8487\n",
            "Epoch 6/30\n",
            "3932/3932 [==============================] - 10s 3ms/step - loss: 0.6960 - accuracy: 0.8492\n",
            "Epoch 7/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.7227 - accuracy: 0.8467\n",
            "Epoch 8/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.7260 - accuracy: 0.8450\n",
            "Epoch 9/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.7435 - accuracy: 0.8435\n",
            "Epoch 10/30\n",
            "3932/3932 [==============================] - 12s 3ms/step - loss: 0.7657 - accuracy: 0.8379\n",
            "Epoch 11/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.7698 - accuracy: 0.8362\n",
            "Epoch 12/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.7949 - accuracy: 0.8312\n",
            "Epoch 13/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.8156 - accuracy: 0.8259\n",
            "Epoch 14/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.8452 - accuracy: 0.8202\n",
            "Epoch 15/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.8516 - accuracy: 0.8182\n",
            "Epoch 16/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.8613 - accuracy: 0.8161\n",
            "Epoch 17/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.8605 - accuracy: 0.8163\n",
            "Epoch 18/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.8746 - accuracy: 0.8122\n",
            "Epoch 19/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.8804 - accuracy: 0.8101\n",
            "Epoch 20/30\n",
            "3932/3932 [==============================] - 12s 3ms/step - loss: 0.8846 - accuracy: 0.8088\n",
            "Epoch 21/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9093 - accuracy: 0.8018\n",
            "Epoch 22/30\n",
            "3932/3932 [==============================] - 12s 3ms/step - loss: 0.8970 - accuracy: 0.8033\n",
            "Epoch 23/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.9111 - accuracy: 0.8000\n",
            "Epoch 24/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9254 - accuracy: 0.7964\n",
            "Epoch 25/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9368 - accuracy: 0.7949\n",
            "Epoch 26/30\n",
            "3932/3932 [==============================] - 10s 3ms/step - loss: 0.9455 - accuracy: 0.7940\n",
            "Epoch 27/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9438 - accuracy: 0.7899\n",
            "Epoch 28/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9588 - accuracy: 0.7871\n",
            "Epoch 29/30\n",
            "3932/3932 [==============================] - 11s 3ms/step - loss: 0.9580 - accuracy: 0.7882\n",
            "Epoch 30/30\n",
            "3932/3932 [==============================] - 13s 3ms/step - loss: 0.9775 - accuracy: 0.7834\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b04306a57b0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "# Membuat model Sequential\n",
        "\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
        "# Menambahkan layer Dense pertama dengan 128 neuron, input_shape sesuai dengan panjang train_x[0], dan fungsi aktivasi ReLU\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "# Menambahkan layer Dropout dengan tingkat dropout 0.5\n",
        "\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# Menambahkan layer Dense kedua dengan 64 neuron dan fungsi aktivasi ReLU\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "# Menambahkan layer Dropout dengan tingkat dropout 0.5\n",
        "\n",
        "model.add(Dense(len(train_y[0]), activation = \"softmax\"))\n",
        "# Menambahkan layer Dense keluaran dengan jumlah neuron sesuai dengan panjang train_y[0] dan fungsi aktivasi softmax\n",
        "\n",
        "adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6)\n",
        "# Menggunakan optimizer Adam dengan learning rate 0.01 dan decay 1e-6\n",
        "\n",
        "# Mengompilasi model dengan fungsi loss categorical_crossentropy, optimizer Adam, dan metrik akurasi\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "# Menampilkan ringkasan model\n",
        "\n",
        "model.fit(x=train_x, y=train_y, epochs=30, verbose=1)\n",
        "# Melatih model dengan data pelatihan (train_x dan train_y) selama 30 epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqU_O1y3_iah"
      },
      "source": [
        "# Building the Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YW5HZ8K_194P"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "# Fungsi untuk membersihkan teks dengan tokenisasi dan lematisasi\n",
        "\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1\n",
        "    return np.array(bow)\n",
        "\n",
        "# Fungsi untuk representasi Bag of Words dari teks berdasarkan vokabuler tertentu\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow])) [0]\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]])\n",
        "  return return_list\n",
        "# Fungsi untuk memprediksi kelas (intent) dari teks menggunakan model\n",
        "\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Maaf! saya tidak mengerti.\"\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result\n",
        "\n",
        "# Fungsi untuk memprediksi kelas (intent) dari teks menggunakan model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8b-WNds_oIk"
      },
      "source": [
        "# Pre-processing the User’s Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jetAmrXM0H_n",
        "outputId": "aaa34e37-a731-46fb-a802-4288c99ce42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press 0 if you don't want to chat with our ChatBot.\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# menampilkan hasil\n",
        "print(\"Press 0 if you don't want to chat with our ChatBot.\")\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  if message == \"0\":\n",
        "    break\n",
        "  intents = pred_class(message, words, classes)\n",
        "  result = get_response(intents, data)\n",
        "  0\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egwZqBFq0IC6",
        "outputId": "1c5d45b0-ebe2-4ee0-c742-7187e9f121d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('chatbot.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jULqDiHs9ez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re5-CzNWs9iT"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_word=2000)\n",
        "tokenizer.fit_on_texts(data['pattern'])\n",
        "train = tokenizer.text_to_sequence(data['pattern'])\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ2RYDfotAS5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
